{
  "snapshots": [
    {
      "step": 1000,
      "mean_intrinsic_reward": 0.12734918312300578,
      "std_intrinsic_reward": 0.2839396891643922,
      "unique_states_seen": 28,
      "state_revisit_rate": 0.972,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.367295829986474,
      "reward_entropy": 1.1338248214373567,
      "exploration_stagnation": 1.0
    },
    {
      "step": 2000,
      "mean_intrinsic_reward": 0.012520812251226743,
      "std_intrinsic_reward": 0.02242846704178811,
      "unique_states_seen": 37,
      "state_revisit_rate": 0.991,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.58351893845611,
      "reward_entropy": 1.2968742701596974,
      "exploration_stagnation": 1.0
    },
    {
      "step": 3000,
      "mean_intrinsic_reward": 0.0036417859847133515,
      "std_intrinsic_reward": 0.0034201829610301998,
      "unique_states_seen": 49,
      "state_revisit_rate": 0.988,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6635616461296463,
      "reward_entropy": 2.0212482748993534,
      "exploration_stagnation": 1.0
    },
    {
      "step": 4000,
      "mean_intrinsic_reward": 0.0014257007365467871,
      "std_intrinsic_reward": 0.0015389058378184747,
      "unique_states_seen": 50,
      "state_revisit_rate": 0.999,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.332204510175204,
      "reward_entropy": 1.8193674022930875,
      "exploration_stagnation": 1.0
    },
    {
      "step": 5000,
      "mean_intrinsic_reward": 0.0012267882773667224,
      "std_intrinsic_reward": 0.0013153160689897171,
      "unique_states_seen": 50,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6635616461296463,
      "reward_entropy": 2.050503903429444,
      "exploration_stagnation": 1.0
    },
    {
      "step": 6000,
      "mean_intrinsic_reward": 0.0007050738671732688,
      "std_intrinsic_reward": 0.0007425580090677572,
      "unique_states_seen": 51,
      "state_revisit_rate": 0.999,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5263605246161616,
      "reward_entropy": 2.231839059392366,
      "exploration_stagnation": 1.0
    },
    {
      "step": 7000,
      "mean_intrinsic_reward": 0.0022711439038921527,
      "std_intrinsic_reward": 0.004743250716564711,
      "unique_states_seen": 54,
      "state_revisit_rate": 0.997,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6109179126442243,
      "reward_entropy": 0.9532582637097461,
      "exploration_stagnation": 1.0
    },
    {
      "step": 8000,
      "mean_intrinsic_reward": 0.0010374233469701722,
      "std_intrinsic_reward": 0.0014170534696255229,
      "unique_states_seen": 56,
      "state_revisit_rate": 0.998,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5553480614894135,
      "reward_entropy": 1.4375793307657252,
      "exploration_stagnation": 0.98
    },
    {
      "step": 9000,
      "mean_intrinsic_reward": 0.0009861567331445257,
      "std_intrinsic_reward": 0.0013778003216287815,
      "unique_states_seen": 61,
      "state_revisit_rate": 0.995,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6375861597263857,
      "reward_entropy": 1.5878280348769511,
      "exploration_stagnation": 1.0
    },
    {
      "step": 10000,
      "mean_intrinsic_reward": 0.00042187411308987067,
      "std_intrinsic_reward": 0.00036402292525568666,
      "unique_states_seen": 69,
      "state_revisit_rate": 0.992,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.8501476017100584,
      "reward_entropy": 2.234513301061927,
      "exploration_stagnation": 0.92
    },
    {
      "step": 11000,
      "mean_intrinsic_reward": 0.0002565495549442858,
      "std_intrinsic_reward": 0.00031260696440926864,
      "unique_states_seen": 69,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.1780538303479458,
      "reward_entropy": 1.8047215078062357,
      "exploration_stagnation": 1.0
    },
    {
      "step": 12000,
      "mean_intrinsic_reward": 0.0003889362262452778,
      "std_intrinsic_reward": 0.00036180517602060264,
      "unique_states_seen": 69,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6635616461296463,
      "reward_entropy": 1.9853385092576064,
      "exploration_stagnation": 1.0
    },
    {
      "step": 13000,
      "mean_intrinsic_reward": 0.00032675761265636537,
      "std_intrinsic_reward": 0.0002575530528874491,
      "unique_states_seen": 69,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5553480614894135,
      "reward_entropy": 2.254180256927301,
      "exploration_stagnation": 1.0
    },
    {
      "step": 14000,
      "mean_intrinsic_reward": 0.00016972110230926773,
      "std_intrinsic_reward": 0.0001552082484196308,
      "unique_states_seen": 69,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.332204510175204,
      "reward_entropy": 2.3556084069072893,
      "exploration_stagnation": 1.0
    },
    {
      "step": 15000,
      "mean_intrinsic_reward": 0.0012004469173843972,
      "std_intrinsic_reward": 0.0017040381203887256,
      "unique_states_seen": 69,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5553480614894135,
      "reward_entropy": 1.2950621620107317,
      "exploration_stagnation": 1.0
    },
    {
      "step": 16000,
      "mean_intrinsic_reward": 0.00040985534751735033,
      "std_intrinsic_reward": 0.0004892930270673938,
      "unique_states_seen": 71,
      "state_revisit_rate": 0.998,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.4339872044851463,
      "reward_entropy": 1.9776551661885367,
      "exploration_stagnation": 1.0
    },
    {
      "step": 17000,
      "mean_intrinsic_reward": 0.0005128202283394785,
      "std_intrinsic_reward": 0.0006051969142573681,
      "unique_states_seen": 71,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.1780538303479458,
      "reward_entropy": 1.7447885995109698,
      "exploration_stagnation": 1.0
    },
    {
      "step": 18000,
      "mean_intrinsic_reward": 0.00025266031938008383,
      "std_intrinsic_reward": 0.00020185930775342954,
      "unique_states_seen": 71,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.4339872044851463,
      "reward_entropy": 2.3772367966678996,
      "exploration_stagnation": 1.0
    },
    {
      "step": 19000,
      "mean_intrinsic_reward": 0.00011586461487036103,
      "std_intrinsic_reward": 0.00011379170532251043,
      "unique_states_seen": 71,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5553480614894135,
      "reward_entropy": 1.672335440048994,
      "exploration_stagnation": 1.0
    },
    {
      "step": 20000,
      "mean_intrinsic_reward": 0.00034915275905041197,
      "std_intrinsic_reward": 0.0005368418093523769,
      "unique_states_seen": 71,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6635616461296463,
      "reward_entropy": 1.2263818143513832,
      "exploration_stagnation": 1.0
    },
    {
      "step": 21000,
      "mean_intrinsic_reward": 0.0002641113095924084,
      "std_intrinsic_reward": 0.00022556616892010643,
      "unique_states_seen": 72,
      "state_revisit_rate": 0.999,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5553480614894135,
      "reward_entropy": 2.2955229483753183,
      "exploration_stagnation": 0.99
    },
    {
      "step": 22000,
      "mean_intrinsic_reward": 0.00039050516508041253,
      "std_intrinsic_reward": 0.0005929282502048173,
      "unique_states_seen": 74,
      "state_revisit_rate": 0.998,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5263605246161616,
      "reward_entropy": 1.2425342204966126,
      "exploration_stagnation": 1.0
    },
    {
      "step": 23000,
      "mean_intrinsic_reward": 0.00024621450866197845,
      "std_intrinsic_reward": 0.0002127156270751001,
      "unique_states_seen": 74,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.1780538303479458,
      "reward_entropy": 2.337379192895532,
      "exploration_stagnation": 1.0
    },
    {
      "step": 24000,
      "mean_intrinsic_reward": 0.0003012527705577668,
      "std_intrinsic_reward": 0.0003038785393444371,
      "unique_states_seen": 74,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.258096538021482,
      "reward_entropy": 1.7192956048952837,
      "exploration_stagnation": 1.0
    },
    {
      "step": 25000,
      "mean_intrinsic_reward": 0.00022885566485092567,
      "std_intrinsic_reward": 0.00019743976468175847,
      "unique_states_seen": 74,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.367295829986474,
      "reward_entropy": 2.037893280813748,
      "exploration_stagnation": 1.0
    },
    {
      "step": 26000,
      "mean_intrinsic_reward": 0.00035229364903989333,
      "std_intrinsic_reward": 0.0005010536797372067,
      "unique_states_seen": 74,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.6888794541139363,
      "reward_entropy": 1.622286031766591,
      "exploration_stagnation": 1.0
    },
    {
      "step": 27000,
      "mean_intrinsic_reward": 0.0007804249328291917,
      "std_intrinsic_reward": 0.0008609344097651905,
      "unique_states_seen": 74,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.8501476017100584,
      "reward_entropy": 1.6989911829690703,
      "exploration_stagnation": 1.0
    },
    {
      "step": 28000,
      "mean_intrinsic_reward": 0.00036001394770664776,
      "std_intrinsic_reward": 0.0004061805841157078,
      "unique_states_seen": 77,
      "state_revisit_rate": 0.997,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.5263605246161616,
      "reward_entropy": 1.8953671664781808,
      "exploration_stagnation": 1.0
    },
    {
      "step": 29000,
      "mean_intrinsic_reward": 0.0001467933577146141,
      "std_intrinsic_reward": 0.00012511810244261103,
      "unique_states_seen": 77,
      "state_revisit_rate": 1.0,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.4965075614664802,
      "reward_entropy": 2.506760250523855,
      "exploration_stagnation": 1.0
    },
    {
      "step": 30000,
      "mean_intrinsic_reward": 0.000530291848230263,
      "std_intrinsic_reward": 0.0005653469191234109,
      "unique_states_seen": 79,
      "state_revisit_rate": 0.998,
      "predictor_loss": 0.0,
      "predictor_gradient_norm": 0.0,
      "embedding_rank": 0.0,
      "state_entropy": 3.784189633918261,
      "reward_entropy": 1.7018002078985717,
      "exploration_stagnation": 1.0
    }
  ],
  "diagnosis": {
    "step": 30000,
    "collapse_detected": true,
    "collapse_step": 500,
    "collapse_diagnosis": "reward_collapse",
    "current_intrinsic_reward": 0.000530291848230263,
    "reward_change": 0.004164077340815368,
    "coverage": 79,
    "exploration_rate": 0.0020000000000000018,
    "predictor_loss_trend": "stable",
    "reward_trend": "stable",
    "coverage_trend": "increasing",
    "recommendations": [
      "Intrinsic rewards near zero. Consider: 1) Adding noise to predictor, 2) Using DRND (multiple networks), 3) Periodic predictor reset",
      "Exploration stagnated. Consider: 1) Increasing epsilon, 2) Go-Explore style returning, 3) Hierarchical exploration",
      "Representation collapse detected. Consider: 1) Adding contrastive loss, 2) Larger embedding dimension, 3) Batch normalization",
      "Gradients vanishing. Consider: 1) Lower learning rate, 2) Gradient clipping, 3) Skip connections"
    ]
  }
}